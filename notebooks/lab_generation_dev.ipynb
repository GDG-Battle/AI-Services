{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b2537a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPowerPointLoader\n",
    "def pptx_parser(file_path):\n",
    "    \"\"\"\n",
    "    Parses a PowerPoint file and returns its content as a list of documents.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the PowerPoint file.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of documents extracted from the PowerPoint file.\n",
    "    \"\"\"\n",
    "    loader = UnstructuredPowerPointLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8eb7f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "available_extentions = ['pptx', 'ppt']\n",
    "\n",
    "def parse_file(file_path, extentions=available_extentions):\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    else:\n",
    "        print(f\"File found: {file_path}\")\n",
    "    # get the file's extension\n",
    "    file_extension = file_path.split('.')[-1].lower()\n",
    "    if file_extension not in extentions:\n",
    "        raise ValueError(f\"Unsupported file extension: {file_extension}\")\n",
    "    if file_extension in ['pptx', 'ppt']:\n",
    "        return pptx_parser(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file extension: {file_extension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b44560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found: ../data/Introduction to generative AI.pptx\n",
      "Introduction to Generative AI\n",
      "\n",
      "\n",
      "\n",
      "Khaoula ALLAK\n",
      "\n",
      "GDG Mentor\n",
      "\n",
      "\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "01\n",
      "\n",
      "What is Generative AI ?\n",
      "\n",
      "02\n",
      "\n",
      "Fundamentals of Large Language Models \n",
      "\n",
      "03\n",
      "\n",
      "How to customize the LLM  ? \n",
      "\n",
      "04\n",
      "\n",
      "Practice\n",
      "\n",
      "\n",
      "\n",
      "01\n",
      "\n",
      "What is Generative AI ?\n",
      "\n",
      "\n",
      "\n",
      "Evolution of AI \n",
      "\n",
      "What matters\n",
      "\n",
      " to us today !\n",
      "\n",
      "\n",
      "\n",
      "Evolution of AI Use Cases\n",
      "\n",
      "Predictive AI\n",
      "\n",
      "Generative AI\n",
      "\n",
      "Multimodal\n",
      "\n",
      "Generative AI\n",
      "\n",
      "Text, Image & Code Generation\n",
      "\n",
      "Text & Code Rewriting & Formatting\n",
      "\n",
      "Summarization\n",
      "\n",
      "Extractive Q&A\n",
      "\n",
      "Image & Video Descriptions\n",
      "\n",
      "Regression & Classification\n",
      "\n",
      "Forecasting\n",
      "\n",
      "Sentiment Analysis\n",
      "\n",
      "Entity Extraction\n",
      "\n",
      "Object Detection\n",
      "\n",
      "Natural Image Understanding \n",
      "\n",
      "Spatial Reasoning and Logic\n",
      "\n",
      "Mathematical Reasoning in Visual Contexts\n",
      "\n",
      "Video Question Answering\n",
      "\n",
      "Automatic Speech Recognition & Translation\n",
      "\n",
      "\n",
      "\n",
      "02\n",
      "\n",
      "Fundamental of LLMs\n",
      "\n",
      "\n",
      "\n",
      "What is LLM? \n",
      "\n",
      "An LLM is a computer program that has been fed enough examples to able to recognize and interpret human language or other types of complex data.\n",
      "\n",
      "LLM are very large models that are pre-trained on vast amounts of data.\n",
      "\n",
      "It can perform different tasks such as answering questions, summarizing documents, translating languages and completing sentences.\n",
      "\n",
      "\n",
      "\n",
      "LLM Architecture \n",
      "\n",
      "Models that convert a sequence of words to an embedding(Vector representation)\n",
      "\n",
      "\n",
      "\n",
      "LLM Architecture \n",
      "\n",
      "Models that take a sequence of words and output next word. (Based on probability).\n",
      "\n",
      "\n",
      "\n",
      "LLM Architecture \n",
      "\n",
      "Encodes a sequence of words and use the encoding to output the next word\n",
      "\n",
      "\n",
      "\n",
      "Prompt & Prompt Engineering\n",
      "\n",
      "Prompt\n",
      "\n",
      "Prompt Engineering\n",
      "\n",
      "The input or initial text provided to the model\n",
      "\n",
      "The process of iteratively refining a prompt for the purpose of eliciting a particular style of response\n",
      "\n",
      "\n",
      "\n",
      "Basic prompt techniques\n",
      "\n",
      "In-context learning: Conditioning an LLm with  instructions and or demonstrations of the task it is meant to complete\n",
      "\n",
      "Summarize this text. Write it as a 3-bullet point summary. Make each sentence short and specific.\n",
      "\n",
      "Use few-shot prompting and experiment with the number of examples \n",
      "\n",
      "[sentence] This is great! //[sentiment] Positive\n",
      "\n",
      "[sentence] This is really bad! //[sentiment] Negative\n",
      "\n",
      "[sentence] That book was fantastic. //[sentiment] Positive\n",
      "\n",
      "[sentence] That show was horrible! //[sentiment]\n",
      "\n",
      "\n",
      "\n",
      "Basic prompt techniques\n",
      "\n",
      "Chain-of-Thought:  prompt the llm to emit intermediate reasoning steps .\n",
      "\n",
      "Solve this problem step by step: If a car travels 60 miles in 2 hours, what is its speed? Here’s how you solve it:\n",
      "\n",
      "Identify the formula for speed: Speed = Distance ÷ Time.\n",
      "\n",
      "Plug in the values: Distance = 60 miles, Time = 2 hours.\n",
      "\n",
      "Perform the calculation: Speed = 60 ÷ 2 = 30 mph.\n",
      "\n",
      "Now solve this: If a car travels 120 miles in 3 hours, what is its speed?\n",
      "\n",
      "The model will mimic your structured reasoning.\n",
      "\n",
      "\n",
      "\n",
      "LLM providers\n",
      "\n",
      "\n",
      "\n",
      "Places to Start Experimentation\n",
      "\n",
      "Hugging Face\n",
      "\n",
      "Vertex AI :Model Garden\n",
      "\n",
      "\n",
      "\n",
      "Places to Start Experimentation\n",
      "\n",
      "Google Colab\n",
      "\n",
      "Google AI Studio\n",
      "\n",
      "\n",
      "\n",
      "Places to Start Experimentation\n",
      "\n",
      "OpenAI Platform \n",
      "\n",
      "Google Cloud skills Boost\n",
      "\n",
      "\n",
      "\n",
      "03\n",
      "\n",
      "How to customize the LLM ?\n",
      "\n",
      "\n",
      "\n",
      "What is RAG? \n",
      "\n",
      "It's an advanced technique\n",
      "\n",
      " used in LLMs.\n",
      "\n",
      " The model retrieves \n",
      "\n",
      " relevant information from\n",
      "\n",
      " a knowledge base or  \n",
      "\n",
      " external sources.\n",
      "\n",
      "\n",
      "\n",
      "Vector Databases\n",
      "\n",
      "\n",
      "\n",
      "Role of Vector Databases with LLms \n",
      "\n",
      "Cheaper than fine-tuning LLMs which can be expensive to update \n",
      "\n",
      "Real-time updated knowledge base\n",
      "\n",
      "Cach previous LLM prompts/responses to improve performance .\n",
      "\n",
      "\n",
      "\n",
      "RAG Pipeline\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "Ingestion\n",
      "\n",
      "Retrieval \n",
      "\n",
      "Generation\n",
      "\n",
      "This is the process of bringing external data into the system\n",
      "\n",
      "Retrieve relevant data from the indexed sources to provide context for the model’s response.\n",
      "\n",
      "Use the retrieved context to generate an accurate, relevant response.\n",
      "\n",
      "\n",
      "\n",
      "  Ingestion\n",
      "\n",
      "\n",
      "\n",
      "Retrieval\n",
      "\n",
      "\n",
      "\n",
      "Generation\n",
      "\n",
      "\n",
      "\n",
      "Let's practice\n",
      "\n",
      "\n",
      "\n",
      "Q&A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "What is Langchain?\n",
      "\n",
      "LangChain is a framework for\n",
      "\n",
      " developing applications\n",
      "\n",
      " powered by language models.\n",
      "\n",
      " It offers a multitude of \n",
      "\n",
      "components that help us\n",
      "\n",
      " build LLM-powered \n",
      "\n",
      "applications.\n",
      "\n",
      "\n",
      "\n",
      "Memory\n",
      "\n",
      "Agents\n",
      "\n",
      "Retaining the entire conversation(store information about past interactions).\n",
      "\n",
      "Use a given language model as a reasoning engine to determine which actions to take .\n",
      "\n",
      "Language agents\n",
      "\n",
      "Output Parsers\n",
      "\n",
      "Are AI models designed to understand, generate, and manipulate human language. They are trained on vast amounts of text data to predict and generate the next word .\n",
      "\n",
      " Interpreting or analyzing the model's output in a structured manner.\n",
      "\n",
      "\n",
      "\n",
      "04\n",
      "\n",
      "Let's practice\n",
      "\n",
      "\n",
      "\n",
      "Learning rag from scratch\n",
      "\n",
      "ai.google.dev/\n",
      "\n",
      "github.com/GoogleCloudPlatform/generative-ai/tree/main/gemini/\n",
      "\n",
      "cloud.google.com/vertex-ai/docs/generative-ai/tutorials\n",
      "\n",
      "Vertex Model Garden\n",
      "\n",
      "Hugging Face\n",
      "\n",
      "Model Garden Community Repo\n",
      "\n",
      "Explore Models in Model Garden\n",
      "\n",
      "Resources\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Conversational chatbot architecture \n",
      "\n",
      "\n",
      "\n",
      "Transformational Moments\n",
      "\n",
      "2014\n",
      "\n",
      "Google started \u000bworking on \u000bML Fairness\n",
      "\n",
      "2017\n",
      "\n",
      "Google is \u000ban AI-first \u000bcompany\n",
      "\n",
      "2021\n",
      "\n",
      "LaMDA chatbot \u000bpaper published\n",
      "\n",
      "2022\n",
      "\n",
      "Transformer-\u000bbased models published: PaLM, PaLM2, Imagen, Parti, Phenaki\n",
      "\n",
      "2022\n",
      "\n",
      "Stable diffusion announced, downloadable\n",
      "\n",
      "2023\n",
      "\n",
      "Google Research and \u000bGoogle DeepMind solidify \u000blarge model efforts in Gemini \u000bprogram for a world class \u000bmultimodal model (Dec)\n",
      "\n",
      "2022\n",
      "\n",
      "Google AI Test \u000bKitchen announces guardrailed LaMDA\n",
      "\n",
      "2017\n",
      "\n",
      "Google publishes Transformer paper\n",
      "\n",
      "2018\n",
      "\n",
      "Google AI Principles published\n",
      "\n",
      "2022\n",
      "\n",
      "ChatGPT \u000bannounced, available \u000bto public (Nov)\n",
      "\n",
      "2023\n",
      "\n",
      "Bard\u000bAnnounced \u000b(Feb)\n",
      "\n",
      "2023\n",
      "\n",
      "Developer access, \u000bCloud access via \u000bAPI at I/O 2023\n",
      "\n",
      "Large, pre-Transformer models (e.g., MUM) applied to many products \u000b(e.g., Google Search, Translate, Maps)\n",
      "\n",
      "\n",
      "\n",
      "Two important choices\n",
      "\n",
      "Proprietary + Confidential\n",
      "\n",
      "What Model?\n",
      "\n",
      "What Infrastructure?\n",
      "\n",
      "🤔\n",
      "\n",
      "\n",
      "\n",
      "Finding the right open model\n",
      "\n",
      "Modality Size Use Case Are you working with text? Code? Image? Video? Is your use case real time? Or can you cache results after processing in batch? Do you need a model with advanced reasoning capabilities (good at Chain-of-Thought, etc)? Or is your task more straightforward (summarization, basic Q&A)\n",
      "\n",
      "\n",
      "\n",
      "Finding the right infrastructure\n",
      "\n",
      "Throughput Latency Size & Budget What volume does your use case need to support? How many users on your application concurrently? How fast does model inference need to be? Is your use case real time (chat bot, agent, etc) How big is your model? Generally, small models (< 10B parameters) can run on an L4 GPU ($) while larger models might need an A100 or TPUs ($$$)\n",
      "\n",
      "\n",
      "\n",
      "Vertex AI is built for developers\n",
      "\n",
      "Extensive quick start library with code samples and jumpstarts for developers of all levels and ecosystems\n",
      "\n",
      "Vertex AI\n",
      "\n",
      "Colab\n",
      "\n",
      "Free developer labs and training resources across Vertex products at Cloud Skills Boost\n",
      "\n",
      "Firebase\n",
      "\n",
      "Interfaces for\n",
      "\n",
      "all developers\n",
      "\n",
      "Robust integrations with popular third party developer tools like Lang Chain, LlamaIndex, Pinecone, and Weaviate.\n",
      "\n",
      "Flutter\n",
      "\n",
      "Packages and extensions to natively support Google Cloud foundation models in Google app developer frameworks like Firebase and Flutter.\n",
      "\n",
      "\n",
      "\n",
      "Google’s Foundation Models on Vertex AI\n",
      "\n",
      "Across a variety of model sizes to address use cases\n",
      "\n",
      "Limited Private GA\n",
      "\n",
      "NEW\n",
      "\n",
      "GA\n",
      "\n",
      "NEW\n",
      "\n",
      "Gemma 2B and 7B\n",
      "\n",
      "Family of lightweight, state-of-the-art open models\n",
      "\n",
      "Gemini 1.5 Pro\n",
      "\n",
      "Multimodal reasoning for longer prompts, 1 million context window\n",
      "\n",
      "Gemini 1.0 Ultra\n",
      "\n",
      " Largest and most capable model for highly complex tasks\n",
      "\n",
      "Gemini 1.0 Pro\n",
      "\n",
      "Multimodal reasoning across a wide range of tasks\n",
      "\n",
      "PaLM for Text / Chat\n",
      "\n",
      "Custom language tasks and multi-turn conversations\n",
      "\n",
      "Imagen 2.0 for Text to Image\n",
      "\n",
      "Create and edit images from \u000bsimple prompts\n",
      "\n",
      "Chirp for \u000bSpeech to Text\n",
      "\n",
      "Build voice enabled applications\n",
      "\n",
      "Codey for \u000bCode Generation\n",
      "\n",
      "Improve coding and debugging\n",
      "\n",
      "NEW\n",
      "\n",
      "NEW\n",
      "\n",
      "Embeddings API for \u000bText and Image\n",
      "\n",
      "Extract semantic information \u000bfrom unstructured data\n",
      "\n",
      "Hugging Face Models \n",
      "\n",
      "Few click deployment to Vertex AI\n",
      "\n",
      "Open Models on Vertex AI\n",
      "\n",
      "Mixtral 8x7B, Image Bind, DITO and more\n",
      "\n",
      "Claude on Vertex AI\n",
      "\n",
      "Claude 2, Instant 1.2, and more\n",
      "-----\n",
      "Total documents parsed: 1\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../data/Introduction to generative AI.pptx\"\n",
    "documents = parse_file(file_path)\n",
    "for doc in documents:\n",
    "    print(doc.page_content)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf38a48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
