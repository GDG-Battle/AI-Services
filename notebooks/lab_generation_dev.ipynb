{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b2537a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPowerPointLoader\n",
    "def pptx_parser(file_path):\n",
    "    \"\"\"\n",
    "    Parses a PowerPoint file and returns its content as a list of documents.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the PowerPoint file.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of documents extracted from the PowerPoint file.\n",
    "    \"\"\"\n",
    "    loader = UnstructuredPowerPointLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8eb7f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "available_extentions = ['pptx', 'ppt']\n",
    "\n",
    "def parse_file(file_path, extentions=available_extentions):\n",
    "    \"\"\"\n",
    "    Parses a file based on its extension and returns its content.\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    # get the file's extension\n",
    "    file_extension = file_path.split('.')[-1].lower()\n",
    "    if file_extension not in extentions:\n",
    "        raise ValueError(f\"Unsupported file extension: {file_extension}\")\n",
    "    if file_extension in ['pptx', 'ppt']:\n",
    "        return pptx_parser(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file extension: {file_extension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97b44560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Introduction to Generative AI\n",
      "\n",
      "\n",
      "\n",
      "Khaoula ALLAK\n",
      "\n",
      "GDG Mentor\n",
      "\n",
      "\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "01\n",
      "\n",
      "What is Generative AI ?\n",
      "\n",
      "02\n",
      "\n",
      "Fundamentals of Large Language Models \n",
      "\n",
      "03\n",
      "\n",
      "How to customize the LLM  ? \n",
      "\n",
      "04\n",
      "\n",
      "Practice\n",
      "\n",
      "\n",
      "\n",
      "01\n",
      "\n",
      "What is Generative AI ?\n",
      "\n",
      "\n",
      "\n",
      "Evolution of AI \n",
      "\n",
      "What matters\n",
      "\n",
      " to us today !\n",
      "\n",
      "\n",
      "\n",
      "Evolution of AI Use Cases\n",
      "\n",
      "Predictive AI\n",
      "\n",
      "Generative AI\n",
      "\n",
      "Multimodal\n",
      "\n",
      "Generative AI\n",
      "\n",
      "Text, Image & Code Generation\n",
      "\n",
      "Text & Code Rewriting & Formatting\n",
      "\n",
      "Summarization\n",
      "\n",
      "Extractive Q&A\n",
      "\n",
      "Image & Video Descriptions\n",
      "\n",
      "Regression & Classification\n",
      "\n",
      "Forecasting\n",
      "\n",
      "Sentiment Analysis\n",
      "\n",
      "Entity Extraction\n",
      "\n",
      "Object Detection\n",
      "\n",
      "Natural Image Understanding \n",
      "\n",
      "Spatial Reasoning and Logic\n",
      "\n",
      "Mathematical Reasoning in Visual Contexts\n",
      "\n",
      "Video Question Answering\n",
      "\n",
      "Automatic Speech Recognition & Translation\n",
      "\n",
      "\n",
      "\n",
      "02\n",
      "\n",
      "Fundamental of LLMs\n",
      "\n",
      "\n",
      "\n",
      "What is LLM? \n",
      "\n",
      "An LLM is a computer program that has been fed enough examples to able to recognize and interpret human language or other types of complex data.\n",
      "\n",
      "LLM are very large models that are pre-trained on vast amounts of data.\n",
      "\n",
      "It can perform different tasks such as answering questions, summarizing documents, translating languages and completing sentences.\n",
      "\n",
      "\n",
      "\n",
      "LLM Architecture \n",
      "\n",
      "Models that convert a sequence of words to an embedding(Vector representation)\n",
      "\n",
      "\n",
      "\n",
      "LLM Architecture \n",
      "\n",
      "Models that take a sequence of words and output next word. (Based on probability).\n",
      "\n",
      "\n",
      "\n",
      "LLM Architecture \n",
      "\n",
      "Encodes a sequence of words and use the encoding to output the next word\n",
      "\n",
      "\n",
      "\n",
      "Prompt & Prompt Engineering\n",
      "\n",
      "Prompt\n",
      "\n",
      "Prompt Engineering\n",
      "\n",
      "The input or initial text provided to the model\n",
      "\n",
      "The process of iteratively refining a prompt for the purpose of eliciting a particular style of response\n",
      "\n",
      "\n",
      "\n",
      "Basic prompt techniques\n",
      "\n",
      "In-context learning: Conditioning an LLm with  instructions and or demonstrations of the task it is meant to complete\n",
      "\n",
      "Summarize this text. Write it as a 3-bullet point summary. Make each sentence short and specific.\n",
      "\n",
      "Use few-shot prompting and experiment with the number of examples \n",
      "\n",
      "[sentence] This is great! //[sentiment] Positive\n",
      "\n",
      "[sentence] This is really bad! //[sentiment] Negative\n",
      "\n",
      "[sentence] That book was fantastic. //[sentiment] Positive\n",
      "\n",
      "[sentence] That show was horrible! //[sentiment]\n",
      "\n",
      "\n",
      "\n",
      "Basic prompt techniques\n",
      "\n",
      "Chain-of-Thought:  prompt the llm to emit intermediate reasoning steps .\n",
      "\n",
      "Solve this problem step by step: If a car travels 60 miles in 2 hours, what is its speed? Here’s how you solve it:\n",
      "\n",
      "Identify the formula for speed: Speed = Distance ÷ Time.\n",
      "\n",
      "Plug in the values: Distance = 60 miles, Time = 2 hours.\n",
      "\n",
      "Perform the calculation: Speed = 60 ÷ 2 = 30 mph.\n",
      "\n",
      "Now solve this: If a car travels 120 miles in 3 hours, what is its speed?\n",
      "\n",
      "The model will mimic your structured reasoning.\n",
      "\n",
      "\n",
      "\n",
      "LLM providers\n",
      "\n",
      "\n",
      "\n",
      "Places to Start Experimentation\n",
      "\n",
      "Hugging Face\n",
      "\n",
      "Vertex AI :Model Garden\n",
      "\n",
      "\n",
      "\n",
      "Places to Start Experimentation\n",
      "\n",
      "Google Colab\n",
      "\n",
      "Google AI Studio\n",
      "\n",
      "\n",
      "\n",
      "Places to Start Experimentation\n",
      "\n",
      "OpenAI Platform \n",
      "\n",
      "Google Cloud skills Boost\n",
      "\n",
      "\n",
      "\n",
      "03\n",
      "\n",
      "How to customize the LLM ?\n",
      "\n",
      "\n",
      "\n",
      "What is RAG? \n",
      "\n",
      "It's an advanced technique\n",
      "\n",
      " used in LLMs.\n",
      "\n",
      " The model retrieves \n",
      "\n",
      " relevant information from\n",
      "\n",
      " a knowledge base or  \n",
      "\n",
      " external sources.\n",
      "\n",
      "\n",
      "\n",
      "Vector Databases\n",
      "\n",
      "\n",
      "\n",
      "Role of Vector Databases with LLms \n",
      "\n",
      "Cheaper than fine-tuning LLMs which can be expensive to update \n",
      "\n",
      "Real-time updated knowledge base\n",
      "\n",
      "Cach previous LLM prompts/responses to improve performance .\n",
      "\n",
      "\n",
      "\n",
      "RAG Pipeline\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "Ingestion\n",
      "\n",
      "Retrieval \n",
      "\n",
      "Generation\n",
      "\n",
      "This is the process of bringing external data into the system\n",
      "\n",
      "Retrieve relevant data from the indexed sources to provide context for the model’s response.\n",
      "\n",
      "Use the retrieved context to generate an accurate, relevant response.\n",
      "\n",
      "\n",
      "\n",
      "  Ingestion\n",
      "\n",
      "\n",
      "\n",
      "Retrieval\n",
      "\n",
      "\n",
      "\n",
      "Generation\n",
      "\n",
      "\n",
      "\n",
      "Let's practice\n",
      "\n",
      "\n",
      "\n",
      "Q&A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "What is Langchain?\n",
      "\n",
      "LangChain is a framework for\n",
      "\n",
      " developing applications\n",
      "\n",
      " powered by language models.\n",
      "\n",
      " It offers a multitude of \n",
      "\n",
      "components that help us\n",
      "\n",
      " build LLM-powered \n",
      "\n",
      "applications.\n",
      "\n",
      "\n",
      "\n",
      "Memory\n",
      "\n",
      "Agents\n",
      "\n",
      "Retaining the entire conversation(store information about past interactions).\n",
      "\n",
      "Use a given language model as a reasoning engine to determine which actions to take .\n",
      "\n",
      "Language agents\n",
      "\n",
      "Output Parsers\n",
      "\n",
      "Are AI models designed to understand, generate, and manipulate human language. They are trained on vast amounts of text data to predict and generate the next word .\n",
      "\n",
      " Interpreting or analyzing the model's output in a structured manner.\n",
      "\n",
      "\n",
      "\n",
      "04\n",
      "\n",
      "Let's practice\n",
      "\n",
      "\n",
      "\n",
      "Learning rag from scratch\n",
      "\n",
      "ai.google.dev/\n",
      "\n",
      "github.com/GoogleCloudPlatform/generative-ai/tree/main/gemini/\n",
      "\n",
      "cloud.google.com/vertex-ai/docs/generative-ai/tutorials\n",
      "\n",
      "Vertex Model Garden\n",
      "\n",
      "Hugging Face\n",
      "\n",
      "Model Garden Community Repo\n",
      "\n",
      "Explore Models in Model Garden\n",
      "\n",
      "Resources\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Conversational chatbot architecture \n",
      "\n",
      "\n",
      "\n",
      "Transformational Moments\n",
      "\n",
      "2014\n",
      "\n",
      "Google started \u000bworking on \u000bML Fairness\n",
      "\n",
      "2017\n",
      "\n",
      "Google is \u000ban AI-first \u000bcompany\n",
      "\n",
      "2021\n",
      "\n",
      "LaMDA chatbot \u000bpaper published\n",
      "\n",
      "2022\n",
      "\n",
      "Transformer-\u000bbased models published: PaLM, PaLM2, Imagen, Parti, Phenaki\n",
      "\n",
      "2022\n",
      "\n",
      "Stable diffusion announced, downloadable\n",
      "\n",
      "2023\n",
      "\n",
      "Google Research and \u000bGoogle DeepMind solidify \u000blarge model efforts in Gemini \u000bprogram for a world class \u000bmultimodal model (Dec)\n",
      "\n",
      "2022\n",
      "\n",
      "Google AI Test \u000bKitchen announces guardrailed LaMDA\n",
      "\n",
      "2017\n",
      "\n",
      "Google publishes Transformer paper\n",
      "\n",
      "2018\n",
      "\n",
      "Google AI Principles published\n",
      "\n",
      "2022\n",
      "\n",
      "ChatGPT \u000bannounced, available \u000bto public (Nov)\n",
      "\n",
      "2023\n",
      "\n",
      "Bard\u000bAnnounced \u000b(Feb)\n",
      "\n",
      "2023\n",
      "\n",
      "Developer access, \u000bCloud access via \u000bAPI at I/O 2023\n",
      "\n",
      "Large, pre-Transformer models (e.g., MUM) applied to many products \u000b(e.g., Google Search, Translate, Maps)\n",
      "\n",
      "\n",
      "\n",
      "Two important choices\n",
      "\n",
      "Proprietary + Confidential\n",
      "\n",
      "What Model?\n",
      "\n",
      "What Infrastructure?\n",
      "\n",
      "🤔\n",
      "\n",
      "\n",
      "\n",
      "Finding the right open model\n",
      "\n",
      "Modality Size Use Case Are you working with text? Code? Image? Video? Is your use case real time? Or can you cache results after processing in batch? Do you need a model with advanced reasoning capabilities (good at Chain-of-Thought, etc)? Or is your task more straightforward (summarization, basic Q&A)\n",
      "\n",
      "\n",
      "\n",
      "Finding the right infrastructure\n",
      "\n",
      "Throughput Latency Size & Budget What volume does your use case need to support? How many users on your application concurrently? How fast does model inference need to be? Is your use case real time (chat bot, agent, etc) How big is your model? Generally, small models (< 10B parameters) can run on an L4 GPU ($) while larger models might need an A100 or TPUs ($$$)\n",
      "\n",
      "\n",
      "\n",
      "Vertex AI is built for developers\n",
      "\n",
      "Extensive quick start library with code samples and jumpstarts for developers of all levels and ecosystems\n",
      "\n",
      "Vertex AI\n",
      "\n",
      "Colab\n",
      "\n",
      "Free developer labs and training resources across Vertex products at Cloud Skills Boost\n",
      "\n",
      "Firebase\n",
      "\n",
      "Interfaces for\n",
      "\n",
      "all developers\n",
      "\n",
      "Robust integrations with popular third party developer tools like Lang Chain, LlamaIndex, Pinecone, and Weaviate.\n",
      "\n",
      "Flutter\n",
      "\n",
      "Packages and extensions to natively support Google Cloud foundation models in Google app developer frameworks like Firebase and Flutter.\n",
      "\n",
      "\n",
      "\n",
      "Google’s Foundation Models on Vertex AI\n",
      "\n",
      "Across a variety of model sizes to address use cases\n",
      "\n",
      "Limited Private GA\n",
      "\n",
      "NEW\n",
      "\n",
      "GA\n",
      "\n",
      "NEW\n",
      "\n",
      "Gemma 2B and 7B\n",
      "\n",
      "Family of lightweight, state-of-the-art open models\n",
      "\n",
      "Gemini 1.5 Pro\n",
      "\n",
      "Multimodal reasoning for longer prompts, 1 million context window\n",
      "\n",
      "Gemini 1.0 Ultra\n",
      "\n",
      " Largest and most capable model for highly complex tasks\n",
      "\n",
      "Gemini 1.0 Pro\n",
      "\n",
      "Multimodal reasoning across a wide range of tasks\n",
      "\n",
      "PaLM for Text / Chat\n",
      "\n",
      "Custom language tasks and multi-turn conversations\n",
      "\n",
      "Imagen 2.0 for Text to Image\n",
      "\n",
      "Create and edit images from \u000bsimple prompts\n",
      "\n",
      "Chirp for \u000bSpeech to Text\n",
      "\n",
      "Build voice enabled applications\n",
      "\n",
      "Codey for \u000bCode Generation\n",
      "\n",
      "Improve coding and debugging\n",
      "\n",
      "NEW\n",
      "\n",
      "NEW\n",
      "\n",
      "Embeddings API for \u000bText and Image\n",
      "\n",
      "Extract semantic information \u000bfrom unstructured data\n",
      "\n",
      "Hugging Face Models \n",
      "\n",
      "Few click deployment to Vertex AI\n",
      "\n",
      "Open Models on Vertex AI\n",
      "\n",
      "Mixtral 8x7B, Image Bind, DITO and more\n",
      "\n",
      "Claude on Vertex AI\n",
      "\n",
      "Claude 2, Instant 1.2, and more' metadata={'source': '../data/Introduction to generative AI.pptx'}\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../data/Introduction to generative AI.pptx\"\n",
    "documents = parse_file(file_path)\n",
    "for doc in documents:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddf38a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/Introduction to generative AI.pptx'}, page_content='Introduction to Generative AI\\n\\n\\n\\nKhaoula ALLAK\\n\\nGDG Mentor\\n\\n\\n\\nTable of Contents\\n\\n01\\n\\nWhat is Generative AI ?\\n\\n02\\n\\nFundamentals of Large Language Models \\n\\n03\\n\\nHow to customize the LLM  ? \\n\\n04\\n\\nPractice\\n\\n\\n\\n01\\n\\nWhat is Generative AI ?\\n\\n\\n\\nEvolution of AI \\n\\nWhat matters\\n\\n to us today !\\n\\n\\n\\nEvolution of AI Use Cases\\n\\nPredictive AI\\n\\nGenerative AI\\n\\nMultimodal\\n\\nGenerative AI\\n\\nText, Image & Code Generation\\n\\nText & Code Rewriting & Formatting\\n\\nSummarization\\n\\nExtractive Q&A\\n\\nImage & Video Descriptions\\n\\nRegression & Classification\\n\\nForecasting\\n\\nSentiment Analysis\\n\\nEntity Extraction\\n\\nObject Detection\\n\\nNatural Image Understanding \\n\\nSpatial Reasoning and Logic\\n\\nMathematical Reasoning in Visual Contexts\\n\\nVideo Question Answering\\n\\nAutomatic Speech Recognition & Translation\\n\\n\\n\\n02\\n\\nFundamental of LLMs\\n\\n\\n\\nWhat is LLM? \\n\\nAn LLM is a computer program that has been fed enough examples to able to recognize and interpret human language or other types of complex data.'),\n",
       " Document(metadata={'source': '../data/Introduction to generative AI.pptx'}, page_content='02\\n\\nFundamental of LLMs\\n\\n\\n\\nWhat is LLM? \\n\\nAn LLM is a computer program that has been fed enough examples to able to recognize and interpret human language or other types of complex data.\\n\\nLLM are very large models that are pre-trained on vast amounts of data.\\n\\nIt can perform different tasks such as answering questions, summarizing documents, translating languages and completing sentences.\\n\\n\\n\\nLLM Architecture \\n\\nModels that convert a sequence of words to an embedding(Vector representation)\\n\\n\\n\\nLLM Architecture \\n\\nModels that take a sequence of words and output next word. (Based on probability).\\n\\n\\n\\nLLM Architecture \\n\\nEncodes a sequence of words and use the encoding to output the next word\\n\\n\\n\\nPrompt & Prompt Engineering\\n\\nPrompt\\n\\nPrompt Engineering\\n\\nThe input or initial text provided to the model\\n\\nThe process of iteratively refining a prompt for the purpose of eliciting a particular style of response\\n\\n\\n\\nBasic prompt techniques'),\n",
       " Document(metadata={'source': '../data/Introduction to generative AI.pptx'}, page_content='The input or initial text provided to the model\\n\\nThe process of iteratively refining a prompt for the purpose of eliciting a particular style of response\\n\\n\\n\\nBasic prompt techniques\\n\\nIn-context learning: Conditioning an LLm with  instructions and or demonstrations of the task it is meant to complete\\n\\nSummarize this text. Write it as a 3-bullet point summary. Make each sentence short and specific.\\n\\nUse few-shot prompting and experiment with the number of examples \\n\\n[sentence] This is great! //[sentiment] Positive\\n\\n[sentence] This is really bad! //[sentiment] Negative\\n\\n[sentence] That book was fantastic. //[sentiment] Positive\\n\\n[sentence] That show was horrible! //[sentiment]\\n\\n\\n\\nBasic prompt techniques\\n\\nChain-of-Thought:  prompt the llm to emit intermediate reasoning steps .\\n\\nSolve this problem step by step: If a car travels 60 miles in 2 hours, what is its speed? Here’s how you solve it:\\n\\nIdentify the formula for speed: Speed = Distance ÷ Time.'),\n",
       " Document(metadata={'source': '../data/Introduction to generative AI.pptx'}, page_content=\"Solve this problem step by step: If a car travels 60 miles in 2 hours, what is its speed? Here’s how you solve it:\\n\\nIdentify the formula for speed: Speed = Distance ÷ Time.\\n\\nPlug in the values: Distance = 60 miles, Time = 2 hours.\\n\\nPerform the calculation: Speed = 60 ÷ 2 = 30 mph.\\n\\nNow solve this: If a car travels 120 miles in 3 hours, what is its speed?\\n\\nThe model will mimic your structured reasoning.\\n\\n\\n\\nLLM providers\\n\\n\\n\\nPlaces to Start Experimentation\\n\\nHugging Face\\n\\nVertex AI :Model Garden\\n\\n\\n\\nPlaces to Start Experimentation\\n\\nGoogle Colab\\n\\nGoogle AI Studio\\n\\n\\n\\nPlaces to Start Experimentation\\n\\nOpenAI Platform \\n\\nGoogle Cloud skills Boost\\n\\n\\n\\n03\\n\\nHow to customize the LLM ?\\n\\n\\n\\nWhat is RAG? \\n\\nIt's an advanced technique\\n\\n used in LLMs.\\n\\n The model retrieves \\n\\n relevant information from\\n\\n a knowledge base or  \\n\\n external sources.\\n\\n\\n\\nVector Databases\\n\\n\\n\\nRole of Vector Databases with LLms \\n\\nCheaper than fine-tuning LLMs which can be expensive to update \\n\\nReal-time updated knowledge base\"),\n",
       " Document(metadata={'source': '../data/Introduction to generative AI.pptx'}, page_content=\"external sources.\\n\\n\\n\\nVector Databases\\n\\n\\n\\nRole of Vector Databases with LLms \\n\\nCheaper than fine-tuning LLMs which can be expensive to update \\n\\nReal-time updated knowledge base\\n\\nCach previous LLM prompts/responses to improve performance .\\n\\n\\n\\nRAG Pipeline\\n\\n1\\n\\n2\\n\\n3\\n\\nIngestion\\n\\nRetrieval \\n\\nGeneration\\n\\nThis is the process of bringing external data into the system\\n\\nRetrieve relevant data from the indexed sources to provide context for the model’s response.\\n\\nUse the retrieved context to generate an accurate, relevant response.\\n\\n\\n\\n  Ingestion\\n\\n\\n\\nRetrieval\\n\\n\\n\\nGeneration\\n\\n\\n\\nLet's practice\\n\\n\\n\\nQ&A\\n\\n\\n\\n\\n\\nWhat is Langchain?\\n\\nLangChain is a framework for\\n\\n developing applications\\n\\n powered by language models.\\n\\n It offers a multitude of \\n\\ncomponents that help us\\n\\n build LLM-powered \\n\\napplications.\\n\\n\\n\\nMemory\\n\\nAgents\\n\\nRetaining the entire conversation(store information about past interactions).\\n\\nUse a given language model as a reasoning engine to determine which actions to take .\\n\\nLanguage agents\"),\n",
       " Document(metadata={'source': '../data/Introduction to generative AI.pptx'}, page_content=\"Memory\\n\\nAgents\\n\\nRetaining the entire conversation(store information about past interactions).\\n\\nUse a given language model as a reasoning engine to determine which actions to take .\\n\\nLanguage agents\\n\\nOutput Parsers\\n\\nAre AI models designed to understand, generate, and manipulate human language. They are trained on vast amounts of text data to predict and generate the next word .\\n\\n Interpreting or analyzing the model's output in a structured manner.\\n\\n\\n\\n04\\n\\nLet's practice\\n\\n\\n\\nLearning rag from scratch\\n\\nai.google.dev/\\n\\ngithub.com/GoogleCloudPlatform/generative-ai/tree/main/gemini/\\n\\ncloud.google.com/vertex-ai/docs/generative-ai/tutorials\\n\\nVertex Model Garden\\n\\nHugging Face\\n\\nModel Garden Community Repo\\n\\nExplore Models in Model Garden\\n\\nResources\\n\\n\\n\\n\\n\\nConversational chatbot architecture \\n\\n\\n\\nTransformational Moments\\n\\n2014\\n\\nGoogle started \\x0bworking on \\x0bML Fairness\\n\\n2017\\n\\nGoogle is \\x0ban AI-first \\x0bcompany\\n\\n2021\\n\\nLaMDA chatbot \\x0bpaper published\\n\\n2022\"),\n",
       " Document(metadata={'source': '../data/Introduction to generative AI.pptx'}, page_content='Conversational chatbot architecture \\n\\n\\n\\nTransformational Moments\\n\\n2014\\n\\nGoogle started \\x0bworking on \\x0bML Fairness\\n\\n2017\\n\\nGoogle is \\x0ban AI-first \\x0bcompany\\n\\n2021\\n\\nLaMDA chatbot \\x0bpaper published\\n\\n2022\\n\\nTransformer-\\x0bbased models published: PaLM, PaLM2, Imagen, Parti, Phenaki\\n\\n2022\\n\\nStable diffusion announced, downloadable\\n\\n2023\\n\\nGoogle Research and \\x0bGoogle DeepMind solidify \\x0blarge model efforts in Gemini \\x0bprogram for a world class \\x0bmultimodal model (Dec)\\n\\n2022\\n\\nGoogle AI Test \\x0bKitchen announces guardrailed LaMDA\\n\\n2017\\n\\nGoogle publishes Transformer paper\\n\\n2018\\n\\nGoogle AI Principles published\\n\\n2022\\n\\nChatGPT \\x0bannounced, available \\x0bto public (Nov)\\n\\n2023\\n\\nBard\\x0bAnnounced \\x0b(Feb)\\n\\n2023\\n\\nDeveloper access, \\x0bCloud access via \\x0bAPI at I/O 2023\\n\\nLarge, pre-Transformer models (e.g., MUM) applied to many products \\x0b(e.g., Google Search, Translate, Maps)\\n\\n\\n\\nTwo important choices\\n\\nProprietary + Confidential\\n\\nWhat Model?\\n\\nWhat Infrastructure?\\n\\n🤔\\n\\n\\n\\nFinding the right open model'),\n",
       " Document(metadata={'source': '../data/Introduction to generative AI.pptx'}, page_content='Two important choices\\n\\nProprietary + Confidential\\n\\nWhat Model?\\n\\nWhat Infrastructure?\\n\\n🤔\\n\\n\\n\\nFinding the right open model\\n\\nModality Size Use Case Are you working with text? Code? Image? Video? Is your use case real time? Or can you cache results after processing in batch? Do you need a model with advanced reasoning capabilities (good at Chain-of-Thought, etc)? Or is your task more straightforward (summarization, basic Q&A)\\n\\n\\n\\nFinding the right infrastructure\\n\\nThroughput Latency Size & Budget What volume does your use case need to support? How many users on your application concurrently? How fast does model inference need to be? Is your use case real time (chat bot, agent, etc) How big is your model? Generally, small models (< 10B parameters) can run on an L4 GPU ($) while larger models might need an A100 or TPUs ($$$)\\n\\n\\n\\nVertex AI is built for developers\\n\\nExtensive quick start library with code samples and jumpstarts for developers of all levels and ecosystems\\n\\nVertex AI\\n\\nColab'),\n",
       " Document(metadata={'source': '../data/Introduction to generative AI.pptx'}, page_content='Vertex AI is built for developers\\n\\nExtensive quick start library with code samples and jumpstarts for developers of all levels and ecosystems\\n\\nVertex AI\\n\\nColab\\n\\nFree developer labs and training resources across Vertex products at Cloud Skills Boost\\n\\nFirebase\\n\\nInterfaces for\\n\\nall developers\\n\\nRobust integrations with popular third party developer tools like Lang Chain, LlamaIndex, Pinecone, and Weaviate.\\n\\nFlutter\\n\\nPackages and extensions to natively support Google Cloud foundation models in Google app developer frameworks like Firebase and Flutter.\\n\\n\\n\\nGoogle’s Foundation Models on Vertex AI\\n\\nAcross a variety of model sizes to address use cases\\n\\nLimited Private GA\\n\\nNEW\\n\\nGA\\n\\nNEW\\n\\nGemma 2B and 7B\\n\\nFamily of lightweight, state-of-the-art open models\\n\\nGemini 1.5 Pro\\n\\nMultimodal reasoning for longer prompts, 1 million context window\\n\\nGemini 1.0 Ultra\\n\\n Largest and most capable model for highly complex tasks\\n\\nGemini 1.0 Pro\\n\\nMultimodal reasoning across a wide range of tasks'),\n",
       " Document(metadata={'source': '../data/Introduction to generative AI.pptx'}, page_content='Gemini 1.0 Ultra\\n\\n Largest and most capable model for highly complex tasks\\n\\nGemini 1.0 Pro\\n\\nMultimodal reasoning across a wide range of tasks\\n\\nPaLM for Text / Chat\\n\\nCustom language tasks and multi-turn conversations\\n\\nImagen 2.0 for Text to Image\\n\\nCreate and edit images from \\x0bsimple prompts\\n\\nChirp for \\x0bSpeech to Text\\n\\nBuild voice enabled applications\\n\\nCodey for \\x0bCode Generation\\n\\nImprove coding and debugging\\n\\nNEW\\n\\nNEW\\n\\nEmbeddings API for \\x0bText and Image\\n\\nExtract semantic information \\x0bfrom unstructured data\\n\\nHugging Face Models \\n\\nFew click deployment to Vertex AI\\n\\nOpen Models on Vertex AI\\n\\nMixtral 8x7B, Image Bind, DITO and more\\n\\nClaude on Vertex AI\\n\\nClaude 2, Instant 1.2, and more')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chunking the documents\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "def chunk_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Splits documents into smaller chunks.\n",
    "\n",
    "    Args:\n",
    "        documents (list): List of documents to be chunked.\n",
    "        chunk_size (int): Size of each chunk.\n",
    "        chunk_overlap (int): Overlap between chunks.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of text chunks.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "# Chunk the documents\n",
    "chunked_documents = chunk_documents(documents)\n",
    "chunked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c57c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing the documents using QDrant\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "import os, dotenv\n",
    "from datetime import datetime\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\", \"http://localhost:6333\")\n",
    "EMBEDDING_MODEL_NAME = \"nv-embed-v1\"\n",
    "\n",
    "def index_documents(documents, collection_name=\"pptx_collection\"+str(int(datetime.now().timestamp())), qdrant_url=QDRANT_URL, embedding_model_name=EMBEDDING_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Indexes documents using QDrant vector store.\n",
    "\n",
    "    Args:\n",
    "        documents (list): List of documents to be indexed.\n",
    "        collection_name (str): Name of the QDrant collection.\n",
    "        qdrant_url (str): URL of the QDrant instance.\n",
    "        embedding_model_name (str): Name of the embedding model to use.\n",
    "\n",
    "    Returns:\n",
    "        Qdrant: The indexed QDrant vector store.\n",
    "    \"\"\"\n",
    "    vectorstore = QdrantVectorStore.from_documents(\n",
    "        documents,\n",
    "        embedding=NVIDIAEmbeddings(model_name=embedding_model_name, nvidia_api_key=os.getenv(\"NVIDIA_API_KEY\")),\n",
    "        collection_name=collection_name,\n",
    "        url=qdrant_url,\n",
    "        prefer_grpc=True,\n",
    "    )\n",
    "    return vectorstore\n",
    "\n",
    "# get vectorstore\n",
    "def get_vectorstore(collection_name=\"pptx_collection\", qdrant_url=QDRANT_URL, embedding_model_name=EMBEDDING_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    get vectorstore with specified collection name and QDrant URL.\n",
    "\n",
    "    Returns:\n",
    "        Qdrant: The QDrant vector store.\n",
    "    \"\"\"\n",
    "    return QdrantVectorStore.from_existing_collection(\n",
    "        collection_name=collection_name,\n",
    "        url=qdrant_url,\n",
    "        embedding=NVIDIAEmbeddings(model_name=embedding_model_name),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e508049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benhima/pi/ai-services/.venv/lib/python3.12/site-packages/langchain_qdrant/qdrant.py:808: UserWarning: Qdrant client version 1.14.2 is incompatible with server version 1.10.1. Major versions should match and minor version difference must not exceed 1. Set check_compatibility=False to skip version check.\n",
      "  client = QdrantClient(**client_options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 10 documents into QDrant collection 'genai'.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "vectorstore = index_documents(chunked_documents, collection_name=\"genai\")\n",
    "print(f\"Indexed {len(chunked_documents)} documents into QDrant collection '{vectorstore.collection_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d9a6e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benhima/pi/ai-services/.venv/lib/python3.12/site-packages/langchain_qdrant/qdrant.py:397: UserWarning: Qdrant client version 1.14.2 is incompatible with server version 1.10.1. Major versions should match and minor version difference must not exceed 1. Set check_compatibility=False to skip version check.\n",
      "  client = QdrantClient(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved vectorstore with collection name: genai\n"
     ]
    }
   ],
   "source": [
    "# get the vectorstore\n",
    "vectorstore = get_vectorstore(collection_name=\"genai\")\n",
    "print(f\"Retrieved vectorstore with collection name: {vectorstore.collection_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ca1b849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Multi-Agent Lab Generation Workflow\n",
    "#\n",
    "# Roles:\n",
    "#  - planner_agent: decide QCM vs coding exercise and difficulty level\n",
    "#  - retriever_agent: split documents, embed chunks, retrieve relevant ones\n",
    "#  - qcm_generator_agent: generate MCQ questions/options/correct_answer\n",
    "#  - code_generator_agent: produce coding exercise description and stub\n",
    "#  - test_generator_agent: create unit tests: input/output pairs\n",
    "#  - executor_agent: run the code against tests and capture results\n",
    "#  - evaluator_agent: check test outcomes; if failure, trigger refinement loop\n",
    "#\n",
    "# Workflow:\n",
    "#  1. load_documents(paths: List[str]) -> raw_texts\n",
    "#  2. chunk_texts(raw_texts) -> chunks\n",
    "#  3. embed_and_store(chunks) in vector DB\n",
    "#  4. planner = planner_agent(user_query, metadata)\n",
    "#     if planner.task == \"qcm\":\n",
    "#         chunks = retriever_agent(planner.topic)\n",
    "#         qcms = qcm_generator_agent(chunks, planner.difficulty)\n",
    "#         evaluator_agent.validate_qcm(qcms)\n",
    "#     elif planner.task == \"code\":\n",
    "#         chunks = retriever_agent(planner.topic)\n",
    "#         stub = code_generator_agent(chunks, planner.difficulty)\n",
    "#         tests = test_generator_agent(stub, chunks)\n",
    "#         result = executor_agent.run_tests(stub, tests)\n",
    "#         evaluator_agent.loop_until_pass(stub, tests, result)\n",
    "#\n",
    "# Each agent should be implemented as a separate function or class method.\n",
    "# Use LangChain + LangGraph for orchestration; vector DB for chunk retrieval;\n",
    "# code sandbox for execution.\n",
    "#\n",
    "# Convention:\n",
    "#  - Clear function signatures for each agent\n",
    "#  - Use meaningful names and type hints\n",
    "#  - Keep each agent focused on its responsibility\n",
    "#\n",
    "# Goals:\n",
    "#  - Modular, testable, and easy-to-debug pipeline\n",
    "#  - Support iterative refinement via evaluator loops\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a4457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lab(user_query: str, task: str, difficulty: str, metadata: dict):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
